(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[486],{8526:function(e,t,n){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/minimax",function(){return n(2123)}])},2123:function(e,t,n){"use strict";n.r(t),n.d(t,{default:function(){return h}});var s=n(5893),a=n(9232),i=n(2685),r=n(2627);let o=e=>{let{children:t}=e;return(0,s.jsx)(i.ZP,{...r.aR,children:t})};function l(e){let t={code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.ah)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.jT,{children:(0,s.jsx)(t.p,{children:"I have been playing a lot of chess recently, and it has gotten me interested\nin how the game should optimally be played. In college I took an intro to AI\ncourse, and one of the first things that we covered was an algorithm called\nminimax, which allows you to discover the optimal play for any two player,\nfully determined zero-sum game. This post will be the first in a series\ndedicated to understanding how to solve games such as this. Later on we will\ndive into Monte Carlo Tree Search (MCTS), reinforcement learning and the use\nof neural networks to determine policies. For now, let's start with minimax."})}),"\n",(0,s.jsx)(t.h2,{children:"Terminology"}),"\n",(0,s.jsx)(t.p,{children:"It is important to stay consistent with terminology, so let's define exactly what we mean by some common terms:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"state"}),": A unique possible configuration of the system in question. For example, in the case of chess this would be the positions of each piece on the board. This could be represented in a variety of ways, for example a 2d array."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"state space"}),": The set of all possible states of a system."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"action"}),": Any move that transitions a system from one state to the next. In chess this would be a valid move resulting in a piece leaving a position ",(0,s.jsx)(t.code,{children:"(a, b)"})," and arriving at the position ",(0,s.jsx)(t.code,{children:"(c, d)"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"heuristic function"}),": A function that takes in the current state and player as input and returns the favorability of that state to the current player."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Later blog posts will build on this set of definitions as we dive into more complex topics."}),"\n",(0,s.jsx)(t.h2,{children:"The Algorithm"}),"\n",(0,s.jsx)(t.p,{children:"The basic idea of minimax is that given the fact that the state space is fully explorable, we can simply play out every possible game, and find out who wins which game. Then we propagate that information back up the constructed tree, choosing the action that led to the most favorable outcome for the current player."}),"\n",(0,s.jsxs)(t.p,{children:["More concretely, we construct a tree where each node is a state and each directed edge represents an action s -> s'. We will use the game of tic-tac-toe for our examples, as it is simple enough to be easily understood but not so trivial that it would be pointless to run minimax. The root of the tree that we construct is just the empty board. The next layer of nodes is each valid move for the first player, ",(0,s.jsx)(t.code,{children:"X"}),". We can use rotational and translational symmetries of the board to exclude some states. Below is the first layer of the tree in question:"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"/static/minimax/basic.png",alt:"Basic",title:"Visual example of first algorithm step"})}),"\n",(0,s.jsxs)(t.p,{children:["Now let's assume that each of these three states have a value associated with them, which represents how good the state is for the player currently at the root of the tree - ",(0,s.jsx)(t.code,{children:"X"}),". Obviously, we assume that ",(0,s.jsx)(t.code,{children:"X"}),' wants to win, which means maximize the "goodness" of the next state. We will lock down exactly how this value is calculated in the future.']}),"\n",(0,s.jsxs)(t.p,{children:["Let's also make the reasonable assumption that the ",(0,s.jsx)(t.code,{children:"O"})," player wants to win too. This means that they want to maximize the favorability of their state as well. Equivalently, we may say that they want to minimize the favorability of a state for player ",(0,s.jsx)(t.code,{children:"X"}),", because it is a two player game. Player ",(0,s.jsx)(t.code,{children:"X"})," should assume that player ",(0,s.jsx)(t.code,{children:"O"})," will play optimally, so that they pick the action that minimizes the value of all the states they can transition to. So player ",(0,s.jsx)(t.code,{children:"X"})," wants to take the state that maximizes its probability of winning over the set of next states, which in turn is a minimum of player ",(0,s.jsx)(t.code,{children:"O"}),"'s probability of winning."]}),"\n",(0,s.jsx)(t.p,{children:"The algorithm goes something like this: construct a tree of all possible states of the game. Start at the leaf nodes of the tree, which occur when one of the players has won the game or there was a tie. We assign a value of 1 if the player whose turn it currently is won the game, -1 if they lost and 0 if the game ended in a tie. These values are then propagated back up the tree recursively, such that a node takes on the maximum value over all its children if it corresponds to a state where it is the current players turn, and the minimum value over its children if the state corresponds to the opposing players turn. In this way the root node should be the value corresponding to the favorability of the current state for the current player, assuming optimal play from both players. The optimal action is simply the action that leads to the optimal value, or the argmax over the child values."}),"\n",(0,s.jsx)(t.p,{children:"To better understand this process, let's go through a visual example with the values being propagated up from the leaves. Below is the first step, where determine for any given row whether we are computing a minimum or maximum over the next states:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"/static/minimax/Abstract-Step-1.png",alt:"Step1",title:"Detailed tree step 1"})}),"\n",(0,s.jsx)(t.p,{children:"Once this is done, we calculate a min over these nodes to get the value of the nodes the next layer up:"}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"/static/minimax/Abstract-Step-1.png",alt:"Step2",title:"Detailed tree step 2"})}),"\n",(0,s.jsxs)(t.p,{children:["Finally, we compute the minimax value of the root node. We choose to take the path that corresponds to that value - in this case the left path. Assuming that player ",(0,s.jsx)(t.code,{children:"O"})," plays optimally, they will also choose the path outlined in green, which minimizes player ",(0,s.jsx)(t.code,{children:"X"}),"'s value. However, if player ",(0,s.jsx)(t.code,{children:"O"})," chooses to play less than optimally, this is even better for X. In this case, player ",(0,s.jsx)(t.code,{children:"X"})," can actually win the game instead of just being able to tie."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{src:"/static/minimax/Abstract-Step-1.png",alt:"Step3",title:"Detailed tree step 3"})}),"\n",(0,s.jsx)(t.h2,{children:"Implementation"}),"\n",(0,s.jsx)(t.p,{children:"We will first define an interface for the type of games that we will be working with. This interface will allow us to design algorithms that are agnostic to the exact details of a particular game. Below is the interface we will use:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"# python\nclass Game:\n  def initial_state(self):\n    '''\n    Return the initial state of the game\n    '''\n    raise NotImplementedError\n\n  def action_space(self, s):\n    '''\n    For any given state returns a list of all possible valid actions\n    '''\n    raise NotImplementedError\n\n  def terminal(self, s):\n    '''\n    Returns whether a given state is terminal\n    '''\n    raise NotImplementedError\n\n  def winner(self, s, p):\n    '''\n    Returns the winner of a game, or -1 if there is no winner\n    '''\n    raise NotImplementedError\n\n  def reward(self, s, p):\n    '''\n    Returns the reward for a given state\n    '''\n    raise NotImplementedError\n\n  def next_state(self, s, a, p):\n    '''\n    Given a state, action, and player id, return the state resulting from the\n    player making that move\n    '''\n    raise NotImplementedError\n\n  def to_readable_string(self, s):\n    '''\n    Returns a pretty-formatted representation of the board\n    '''\n    return str(s)\n\n  def to_hash(self, s):\n    '''\n    Returns a hash of the game state, which is necessary for some algorithms\n    such as MCTS\n    '''\n    return hash(str(s))\n"})}),"\n",(0,s.jsx)(t.p,{children:"Now let's implement Tic Tac Toe using this interface."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"# python\nclass TicTacToe(Game):\n  def initial_state(self):\n    return [-1 for i in range(9)]\n\n  def action_space(self, s):\n    return [i for i in range(len(s)) if s[i] == -1]\n\n  def terminal(self, s):\n    return self.is_winner(s, 0) or self.is_winner(s, 1) or len(self.action_space(s)) == 0\n\n  def winner(self, s):\n    if self.is_winner(s, 0): return 0\n    if self.is_winner(s, 1): return 1\n    return -1\n\n  def reward(self, s, p):\n    if self.is_winner(s, p): return 1\n    if self.is_winner(s, 1-p): return -1\n    return self.heuristic(s)\n\n  def heuristic(self, s):\n    ''' Stubbed for now '''\n    return 0\n\n  def next_state(self, s, a, p):\n    copy = s.copy()\n    copy[a] = p\n    return copy\n\n  def to_readable_string(self, s):\n    board = \"\"\n    for i, player in enumerate(s):\n        end_of_line = (i + 1) % math.sqrt(len(s)) != 0\n        row_line = \"\\\\n-----------\\\\n\" if i != len(s) - 1 else \"\"\n        board += \" {} {}\".format(self.stringify_player(player), \"|\" if end_of_line else row_line)\n    return board\n\n  def to_hash(self, s):\n    return hash(tuple(s))\n\n  def is_winner(self, s, p):\n    '''\n    Return whether a particular player has won the game. Ideally this would\n    be generalized to an nxn board.\n    '''\n    return ((s[6] == p and s[7] == p and s[8] == p) or\n    (s[3] == p and s[4] == p and s[5] == p) or\n    (s[0] == p and s[1] == p and s[2] == p) or\n    (s[6] == p and s[3] == p and s[0] == p) or\n    (s[7] == p and s[4] == p and s[1] == p) or\n    (s[8] == p and s[5] == p and s[2] == p) or\n    (s[6] == p and s[4] == p and s[2] == p) or\n    (s[8] == p and s[4] == p and s[0] == p))\n\n  def stringify_player(self, tile):\n    mapping = dict(enumerate(['X', 'O']))\n    return mapping.get(tile, ' ')\n"})}),"\n",(0,s.jsx)(t.p,{children:"Minimax can be implemented quite elegantly with recursion. For simplicity, we define a player with either a 1 or a 0. The implementation makes no assumptions about the way state is stored - this decision takes place within the Game class logic."}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-python",children:"# python\ndef minimax(g, s, p):\n  actions = g.action_space(s)\n  rewards = [min_play(g, g.next_state(s, a, p), 1-p) for a in actions]\n  return actions[rewards.index(max(rewards))]\n\ndef min_play(g, s, p):\n  actions = g.action_space(s)\n  if g.terminal(s): return g.reward(s, 1-p)\n  return min([max_play(g, g.next_state(s, a, p), 1-p) for a in actions])\n\ndef max_play(g, s, p):\n  actions = g.action_space(s)\n  if g.terminal(s): return g.reward(s, p)\n  return max([min_play(g, g.next_state(s, a, p), 1-p) for a in actions])\n"})}),"\n",(0,s.jsx)(t.p,{children:"A complete implementation of solving Tic Tac Toe using minimax can be found here. I have also started a repo where I will be implementing different games and algorithms here - feel free to contribute!"}),"\n",(0,s.jsx)(t.h2,{children:"Findings"}),"\n",(0,s.jsx)(t.p,{children:"The entire size of the tic-tac-toe state space is only 5,478 [1]. This is easily traversed by our minimax algorithm, so it should be producing the optimal play for each player. Congratulations! You just solved tic-tac-toe! So what are the results? Who wins? Well, it turns out that optimal play results in a draw. Every. Single. Time. Funnily enough, many people think that the optimal solution to chess is also a draw, although the state space of chess is so much larger that this has yet to be confirmed. Next time your friend says they're unbeatable at tic-tic-tac toe, you can prove them wrong. Probably won't ever happen, but hey it can't hurt to be prepared."}),"\n",(0,s.jsx)(t.h2,{children:"Improvements"}),"\n",(0,s.jsx)(t.h3,{children:"Limited Horizon Search"}),"\n",(0,s.jsx)(t.p,{children:"Because it is often not feasable to fully explore the state space, (games like chess have a high branching factor), we can modify the algorithm to only construct the tree to a certain depth. Then instead of evaluating the leaf nodes based on the win/loss condition, we evalutate them based on a heuristic function that is supposed to be an estimate of how favorable a given state is to the current player. We can modify our implementation to do this with the following changes:"}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"Modify the base cases of min_play and max_play to also return the reward after a certain depth"}),"\n",(0,s.jsx)(t.li,{children:"Modify the reward to return the value of some heurisitic instead of just [-1, 0, 1]."}),"\n"]}),"\n",(0,s.jsx)(t.h3,{children:"Alpha Beta Pruning"}),"\n",(0,s.jsx)(t.p,{children:"It turns out that the minimax algorithm visits a lot of unessesary states when performing its calculations. We can mitigate this using alpha-beta pruning, which eliminates branches of the tree that cannot be optimal based on the current stored node values. This improvement can greatly increase the speed at which the algorithm runs, while still returning optimal results."}),"\n",(0,s.jsx)(t.p,{children:"In my next post I will dive into these two topics and describe an optimal strategy for tic-tac-toe play. If you want a challenge, try implementing limited horizon search yourself!"})]})}function h(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return(0,s.jsx)(o,{...e,children:(0,s.jsx)(l,{...e})})}}},function(e){e.O(0,[869,34,685,774,888,179],function(){return e(e.s=8526)}),_N_E=e.O()}]);