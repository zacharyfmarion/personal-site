import Post, { OpeningParagraph } from "@/components/Post";
import TreeAnimation from "@/components/TreeAnimation";
import MinimaxAnimation from "@/components/MinimaxAnimation";
import Arrow from "@/components/Arrow";

export const metadata = {
  author: "Zachary Marion",
  title: "Minimax Continued",
  date: "09-12-2018",
  description:
    "So in between my last post and this one, I realized that I wanted to create some tree animations to show some common tree operations and explain more gracefully how the minimax algorithm worked.",
  image:
    "https://images.unsplash.com/photo-1465487862947-ded619a2a9ab?ixlib=rb-0.3.5&q=80&fm=jpg&crop=entropy&cs=tinysrgb&w=1080&fit=max&ixid=eyJhcHBfaWQiOjExNzczfQ&s=68130b7d4392394e1876bbd54c82f427",
  link: "/posts/minimax-continued",
  readTime: 12,
};

export default ({ children }) => <Post {...meta}>{children}</Post>;

<OpeningParagraph>
  Our minimax algorithm covered in the previous blog post will consistently
  produce the optimal play for each player. There's only one problem - it's
  extremely slow. For a game of tic-tac-toe, with a relatively small state
  space, this isn't a problem. However, for games like Chess and even Othello,
  the state space becomes so large that running minimax quickly becomes
  impractical. Running minimax on the opening state of a chess board would take
  hundreds of thousands if not millions of years to complete on consumer
  hardware. And you would run out of memory long before that. So how do we get
  around the problem of indeterminate runtimes due to game complexity? There are
  many improvements that can be made to the base minimax algorithm. In this
  post, I will discuss two at length. Firstly, we will explore limiting the
  depth of the algorithm, and secondly we will investigate alpha-beta pruning,
  an optimization of minimax that still ensures the algorithm's optimality.
</OpeningParagraph>

## Depth Limited Minimax

As a quick reminder, the term "heuristic" is used to describe a function that approximates the favorabilty of a given state to the current player. For example, in chess a very naive heuristic could be the number of pieces the current player has minus the number of pieces the other player has. If this number is positive, the player is more likely to be winning, and if it is negative they are more likely to be losing. Actual chess engines use much more complex heuristics, but the basic concept is the same.

Assuming that we cannot actually traverse the entire state tree, we can simply traverse a fixed depth instead. Now our termination condition is either of the following:

- The game is over (win, loss, or draw)
- The maximum depth from the current state was reached

From here we score the leaf nodes based on a heuristic function, and perform the exact same minimax algorithm based on these values. It is important to note that a heuristic function's value for a given state does not necessary map correctly to the true minimax value of the state (assuming the entire state tree had been traversed). This means that suboptimal play could be produced by a depth-limited minimax algorithm. However, limiting the depth does allow us to apply minimax to a much broader set of games (such as chess) and still produce relatively good agents.

## Alpha Beta Pruning

### Background

It turns out that the minimax algorithm visits a lot of unnnecessary states during its traversal. To be precise, it visits many states that could not correspond to the optimal play, given the current information available. To understand why this is, we first need to parse out the recursive execution of the algorithm. When explaining it before, I made it seem as though we found all the leaf nodes first, and then computed the next layer of values from these nodes. However, this is not actually how the algorithm executes. Minimax is recursive by nature, and so its execution essentially maps to a depth-first traversal of the state tree. If this is not familiar, a depth first seach can be explained best visually:

<TreeAnimation
  rootNode="O"
  vertexMap={
    new Map([
      ["O", ["E", "F", "N"]],
      ["E", ["A", "D"]],
      ["F", []],
      ["N", ["G", "M"]],
      ["A", []],
      ["D", ["B", "C"]],
      ["G", []],
      ["M", ["H", "I", "J", "K", "L"]],
      ["B", []],
      ["C", []],
      ["H", []],
      ["I", []],
      ["J", []],
      ["K", []],
      ["L", []],
    ])
  }
/>

Assuming we are going left to right, we essentially go all the way down the left side of the tree until we reach a leaf node. Once we reach that node, we go back up until we find a node that has other unvisited children. Once that node is found we repeat our previous step on that subtree, going all the way down the left until we reach a leaf, and then back up until we find a node with more univisted children. Eventually we will have reached all of the nodes in the tree, completing our traversal.

Minimax essentially does this traversal, with the following modification: we go down the left of the tree until we reach a leaf. Score that leaf based on the outcome of the game (or the heuristic if we are doing a depth limited search). Then go back up the parent. If the parent's children have all been visited, then set the parent's value to the min or max of its children (depending on which player it is). If the parent has unvisited children, visit the leftmost child and repeat the above process. Eventually the value of the root will be set and the traversal is complete. Feel free to play around with the animation of this process to get a feel for how it works:

<MinimaxAnimation
  rootNode="O"
  vertexMap={
    new Map([
      ["O", ["E", "F", "N"]],
      ["E", ["A", "D"]],
      ["F", []],
      ["N", ["G", "M"]],
      ["A", []],
      ["D", ["B", "C"]],
      ["G", []],
      ["M", ["H", "I", "J", "K", "L"]],
      ["B", []],
      ["C", []],
      ["H", []],
      ["I", []],
      ["J", []],
      ["K", []],
      ["L", []],
    ])
  }
  rewards={
    new Map([
      ["F", 1],
      ["A", 5],
      ["G", 4],
      ["B", -1],
      ["C", 3],
      ["H", -4],
      ["I", 9],
      ["J", 2],
      ["K", -9],
      ["L", 1],
    ])
  }
/>

Note that we are computing the max and min of the children in a subtly different way here. Instead of creating an array of the child values and taking the min or max of that, we iterate over the children of a node in some specific order. For example when computing the max of the nodes in the bottom right ([-4, 1, 2, -9, 9]), we go from left to right, keeping track of the highest value we have seen.

### The Algorithm

With this in mind, we can now jump into alpha-beta pruning. Looking at the same example, we are actually visiting some states unnnecessarily. Zooming in on the left subtree of our graph,

<MinimaxAnimation
  alphaBeta
  rootNode="O"
  vertexMap={
    new Map([
      ["O", ["E", "F", "N"]],
      ["E", ["A", "D"]],
      ["F", []],
      ["N", ["G", "M"]],
      ["A", []],
      ["D", ["B", "C"]],
      ["G", []],
      ["M", ["H", "I", "J", "K", "L"]],
      ["B", []],
      ["C", []],
      ["H", []],
      ["I", []],
      ["J", []],
      ["K", []],
      ["L", []],
    ])
  }
  rewards={
    new Map([
      ["F", 1],
      ["A", 5],
      ["G", 4],
      ["B", -1],
      ["C", 3],
      ["H", -4],
      ["I", 9],
      ["J", 2],
      ["K", -9],
      ["L", 1],
    ])
  }
/>

It is important to note that the states that are considered unnnecessary are entirely dependent on the traversal order. For example, If we visited the children of node K [TODO THIS DOESN'T WORK HERE] (the one with the value 9) in the order `[-4, 2, -9, 1, 9]`, none of the would be unnecessary as the only value we find that is greater than 4 is the final value, 9.
